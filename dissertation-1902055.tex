\documentclass[journal]{IEEEtran}
\usepackage{algorithmic}
\usepackage{amsmath,amsfonts}
\usepackage{array}
\usepackage{balance}
\usepackage{biblatex}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\graphicspath{{./figures/}}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{stfloats}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{url}
\usepackage{verbatim}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\addbibresource{references.bib}

\begin{document}
\title{The automated detection of fraudulent peer-to-peer transactions in massively multiplayer online economies}
\author{Hayley Davies - 1902055}

\maketitle
\footnote{Source code for all algorithms is available on GitHub: \url{https://github.com/cdgamedev/dissertation}}
\footnote{Source \LaTeX is available on Falmouth GitHub: \url{https://github.falmouth.ac.uk/Games-Academy-Student-Work-22-23/1902055-comp3xx-dissertation}}

\begin{abstract}
Massively Multiplayer Online games are a hugely popular and successful genre within the gaming industry. These games allow players to trade items within the game, but some players choose to buy and sell in-game items for real-world money, known as Real Money Trading. This leads to people preferring to buy from illicit sources rather than through the game itself, resulting in a loss of revenue for the developers. Additionally, this practice enables people to make money through methods such as botting, account theft, or cheating.
\end{abstract}

\begin{IEEEkeywords}
Anomaly Detection, Massively Multiplayer Online, Real Money Trading
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{R}{eal-Money} Trading (RMT) is the practice of buying and selling virtual items and currency within massively multiplayer online (MMO) video games. This practice often violates the games' Terms of Service or Code of Conduct\cite{AmazonGamesCOC}\cite{SquareEnixCOC}, and as a result, players who engage in RMT are often at risk of being banned from the game. RMT has negative effects on the game's economy and community, and it is therefore discouraged by game developers and players alike. The solution to this issue is anomaly detection algorithms. These are useful in serveral facets of computer science and this paper will discuss the use of these algorithms and compare different anomaly detection algorithms which utilise Standard Deviation.

\section{Literature Review}
\subsection{Anomaly Detection for RMT}
\noindent Anomaly detection is a technique used to help address the issue of RMT in MMOs\cite{Tao2019}\cite{Ahmad2009}. By using computers to flag in-game transactions automatically for human review, it is possible to identify and prevent RMT activity in a way where humans don't have to analyse the all the data manually by hand. Preventing RMT helps to protect the game's economy and maintain a fair and balanced playing experience for all players. However, it is important to note that the effectiveness of this approach will depend on the specific details of the game and the methods used for detecting anomalies.

Fujita et al. propose a method for addressing the issue of RMT in MMO games, which involves identifying suspects, verifying their involvement in RMT activity, and banning their accounts\cite{Fujita2011}. They also classify RMT players into three categories: 
\begin{itemize}
    \item \textit{Sellers} are those who sell the virtual property to players for real-world money.
    \item \textit{Earners} acquire virtual property (currency and items) from non-player characters (NPCs) and real players.
    \item \textit{Collectors} convey virtual property from earners to sellers.
\end{itemize}

Fujita et al. manually classified a set of players and then used an algorithm proposed by Newman and Girvan\cite{Girvan2002} to extract communities and further identify players. They ranked players based on the number of times they traded currency, the number of trades they made in total, and the total volume of currency traded.

Fujita et al. argue that RMT is harmful to both the game's economy and its players, as it is often associated with other illicit activities such as cheating, botting, and account theft. RMT can also drive away legitimate players who become frustrated with the problems it causes, and it can discourage new players from joining the game\cite{Sifa2021}. Overall, Fujita et al. believe that RMT is a serious issue that needs to be addressed to protect the integrity and health of MMO games. Han et al. and Sifa et al. back up Fujita's claims adding that "cheating in MMOs often reduces the shelf life of the game" by causing people to abandon it\cite{Han2022}\cite{Sifa2021}.

\subsection{Types of Anomaly}
\noindent Anomalous data can typically be categorized in three different ways\cite{Ahmed2016}\cite{Bhuyan2013}. These categories are:
\begin{itemize}
    \item \textbf{Point Anomaly} where a data point is unusually out of range.
    \item \textbf{Contextual Anomaly} (or collective anomaly\cite{Song2007}) where sometimes, a data point which seems anomalous is actually within range depending on a varying factors.
    \item \textbf{Collective Anomaly} is where multiple data points are out of range, but when considered individually, are not out of range.
\end{itemize}
Understanding these categories can help researchers identify and address potential issues in their data. By detecting anomalies, it may be possible to uncover hidden patterns or trends and improve the accuracy and reliability of data-driven systems and models.

\subsection{Machine Learning}
\noindent Due to the nature of anomaly detection is often done with Machine Learning (ML) algorithms\cite{Omar2013}\cite{Misra2020}\cite{Zong2018}. These algorithms are split into two categories, supervised and unsupervised\cite{Omar2013}. Supervised methods "require a labeled training set containing both normal and anomalous samples", whereas, unsupervised methods don't require training data as they assume a fraction of data points are anomalous\cite{Omar2013}.

Nassif et al. state that they "recommend that researchers conduct more research on ML studies of anomaly detection to gain more evidence on ML model performance and efficiency" following their own review of prior research. They mention that unsupervised datasets have a greater number of research papers than supervised datasets. They also identify 29 different machine learning models for detecting anomalies\cite{Nassif2021}.

\subsection{Nearest-neighbor Based Algorithms}
\noindent Local Outlier Factor (LOF) works by getting each object in the dataset to "indicate its [own] degree of outlier-ness"\cite{Breunig2000}.

k-Nearest Neighbor (kNN) is a supervised learning algorithm based on Nearest Neighbor and it's discussed frequently within anomaly detection and ties in with lazy learning algorithms. kNN functions by finding the K number of the nearest points and assigns the data point to a label that is most suited, this can be used for anomaly detection if the assignment is different to how the data is already labelled\cite{Cover1967}.

Lazy learning algorithms work by\cite{Wettschereck1997}:
\begin{itemize}
    \item Storing all training data, and deferring processing until queries are given the required replies.
    \item Answering queries by combining the training data.
    \item After replying, the answer and any results are discarded.
\end{itemize}

Many improvements to the base kNN algorithm have been developed by other researchers. Muti-label kNN (ML-kNN) exists to provide lazy learning to problems such as text categorization or bioinformatics. This approach works by \cite{Zhang2005}.

\subsection{Graph-based Anomaly Detection}
\noindent Graph-based anomaly detection algorithms "[detect] patterns (substructures) within graphs" where a "substructure is a connected subgraph in the overall graph"\cite{Noble2003}. Noble et al. utilized their anomaly detection, Subdue, for intrusion detection, utilizing data which "contained 41 features describing the connection" and labelling them as "one of 37 different attack types" or "normal". This is a form of unsupverised learning\cite{Noble2003}. Graph-based anomaly detection works well with large datasets and is often used for collective anomalies\cite{Egilmez2014}.

Davis et al. discuss the use of Yet Another Graph-based Anomaly Detection Algoritm (YAGADA). They find that in other methods of anomaly detection, single time events are detected easily, but anomalous patterns of data which are anomalous in context such as "an airport technician who regularly hangs around in the baggage handling area, or a clerk who is spending an unusually long time on their own
in the cash room" would not normally be detected. They conclude that YAGADA works well for static graphs and suggest using it in "forensic analysis of graph transaction databases". They also conclude that LOF\cite{Breunig2000} is more suitable to numeric anomaly detection.

\subsection{Autoencoders}
\noindent Misra et al. focus on an autoencoder based model for detecting fraudulent transactions within the financial domain, specifically credit cards\cite{Misra2020}. They propose a two stage method where "a lower dimension of features are extracted from the input" before "a model decides whether the transaction is fraud or not". The first stage utilizes an autoencoder and the final stage utilizes a classification algorithm. "Autoencoders are simple learning circuits which aim to transform inputs into outputs with the least possible amount of distortion"\cite{Baldi2011}. They state that having too many features can cause classification algorithms to "run poorly" and that the "data becomes very expensive [when] time complexity is concerned" and is resolved by reducing the number of features. Misra defined features or attributes as parts of a whole data point and that autoencoders can extract these features nicely on any dataset. For credit card fraud, some of these attributes are, time/amount/mode/location of transaction, a user's account number, a user's age.

Deep Autoencoding Gaussian Mixture Model (DAGMM) works by preserving "information of an input sample in a low-dimensional space" and then performs a "Gaussian Mixture Model over the learned low-dimensional space" before utilizing "a sub-network called estimation network that takes the low-dimensional input from the compression network and outputs mixture membership prediction for each sample"\cite{Zong2018}.

\subsection{Standard Deviation}
\noindent Yang et al. discuss the use of standard deviation (SD) within anomaly detection\cite{Yang2019}. However, they note that simple datasets can cause false positives\cite{Pollet2017} and researchers misuse SD methods frequently\cite{Simmons2016}. The main issue with SD algorithms is that outliers influence the standard deviation and averages for the dataset.

To solve these issues Yang set out to develop a modified SD algorithm which could be relied on to give more accurate results. Two-stage thresholding (2T)\cite{Yang2019} works similarly to Clever Standard Deviation (Clever SD)\cite{Buzzi2011} by utilizing recursion. 2T works by recursively removing outliers one at a time and is the most accurate method of SD algorithms based on Yang's findings.

\subsection{Anomaly Detection for Other Datasets}
\noindent Bergman and Hoshen talk about anomaly detection for general data and classification of anomalies using AI. Examples, of where this is used, are for fraudulent credit transactions and detecting cyber attacks amongst others\cite{Bergman2020}. They state that "classification-based methods have dominated supervised anomaly detection", these are methods of anomaly detection which utilise a classifier trained by an ML model. They further discuss the use of the following semi-supervised methods; one-class classification and geometric-transformation classification. They make a comparison between SVMs\cite{Schoelkopf1999}, LOF\cite{Breunig2000} and DAGMM\cite{Zong2018}.

Similarly, Misra and Sadineni investigate the use of anomaly detection within Credit Card transactions\cite{Misra2020}\cite{Sadineni2020}.

\section{Research Questions}
\noindent The questions this paper sets out to answer are:
\begin{itemize}
    \item Which anomaly detection algorithm is the most performant for peer-to-peer transactions within MMOs?
    \item Which algorithm will detect the most anomalous data points?
\end{itemize}

By answering these, game developers working on MMO titles can easily identify and choose a method that suits the needs for their game. This will help reduce the revenue loss caused by RMT which could be millions\cite{Dibbell2007}.

\section{Hypotheses}
\subsection{Which anomaly detection algorithm is the most performant for peer-to-peer transactions within MMOs?}
\noindent
Hypothesis: Following research conducted, for data specifically from Lost Ark, a simple method which doesn't require training data is most likely to be best. The 2T algorithm\cite{Yang2019} could be beneficial for a dataset which has a small number of features as the data from Lost Ark requires only 2 features, a more complex algorithm likely isn't required.

\noindent
Data Source: The average compute time of each algorithm compared.

\subsection{Which algorithm will detect the most anomalous data points?}
\noindent
Hypothesis: Due to the fact that the dataset used cannot be labelled, it will be difficult to quantify if the algorithms function to detect all fraudulent transactions in the dataset. Instead by looking at the quantity, rather than quality, we can assess if an algorithm successfully identifies more or less anomalous transactions than another algorithm. I believe that CleverSD will find the most anomalous transactions due to it processing the data recursively.

\noindent
Data Source: The number of anomalous transactions found by each algorithm.

\section{Computing Artefact}
\subsection{Game Background}
\noindent For this research, the primary focus will be on Smilegate and Amazon Games' Fantasy MMORPG; Lost Ark\cite{LostArk2019} with a specific focus on its Gem system. Gems are collected by players during gameplay and can be sold to others using the in-game marketplace. Each Gem has different attributes, such as, Level, Name, Tier, Gem Effect, Sale Price and Sale Date.
\begin{itemize}
    \item \textit{Level} is a number between 1 and 10 which impact the effect of the Gem. Level \( n \) gems are created by merging 3 Level  \( n-1 \) gems. A Level 10 gem should always cost more than a Level 1 gem as it takes \( 3^{10} \) Level 1 gems to make a single Level 10 gem.
    \item \textit{Name} is a tag given to the gem and doesn't effect the gem. This means that name shouldn't affect the price.
    \item \textit{Tier} is a number between 1 and 3 for all items in Lost Ark, however, gems only exist at the start of Tier 2 meaning all gems will either be tagged with Tier 2 or Tier 3. Tier 3 gems get unlocked at Item Level 1302 and offer higher effects than their Tier 2 counterparts. This means that a Level 1 Tier 2 gem should cost less than a Level 1 Tier 3 gem, however, a Level 10 Tier 2 gem is likely to cost more than a Level 1 Tier 3 gem.
    \item \textit{Gem Effect} is the overall effect of the gem. This will either, increase the damage or decrease the cooldown of an ability in the game. Various gem effects will be more favourable based on the specific character build a player chooses. Favourable gem choices depend on the games meta and players often utilize a service like Maxroll\cite{Maxroll2022} to get the best build for their characters class. Gem Effect can have an impact on the price of a gem, however, this data is near impossible to get without direct access to the internal marketplace database. However, gems can also be rerolled for silver which means that there shouldn't be a huge price disparity between two Level 1 or two Level 10 gems of the same tier.
    \item \textit{Sale Price} is the price which a gem was sold for.
    \item \textit{Sale Date} is the date which a gem sold on.
\end{itemize}

For this research, an anomaly detection algorithm would utilize 4 factors; Level, Tier, Sale Price and Sale Date. This should be enough for a basis to detect prices which are too high at any given time. It also allows us to investigate fluctuations in price of gems over time which could be due to various factors in the game, such as events which inflate the number of gems within the game through increased drop rates of gems, gem giveaways or a change in the number of players which leads to less supply/demand.

Whilst looking at gem data, the average price per gem can be seen in Figure \ref{graph:gem-sale-price}. This graph also shows the upper and lower bounds for gem sale price.
\begin{figure*}[!h]
    \centering
    \includegraphics[width=12cm]{graph-gem-sale-prices.png}
    \caption{Box plot showing the price variation of each gem ID. R shown in Figure \ref{algorithm:r-gem-sale-price}}
    \label{graph:gem-sale-price}
\end{figure*}

\subsection{Detection Algorithms}
For this paper, 4 different algorithms were written which can detect anomalies. These revolve around checking each data point and if the deviance of that data point is within a specific range. All results from these algorithms, use a constant threshold which scales to the remaining dataset. This initial threshold has not been tailored per algorithm.

\subsubsection{Two Stage Thresholding Algorithm}
Figure \ref{algorithm:2t} showcases the 2T algorithm written for use within this paper. This algorithm is recursive and follows the general function outlined by Yang\cite{Yang2019} in their original algorithm. This algorithm works similarly to the Mean Standard Deviation with its main change being that it runs recursively.

\subsubsection{Clever SD Algorithm}
Figure \ref{algorithm:clever-sd} showcases the Clever SD algorithm written for use within this paper. This algorithm is recursive and follows the general function outlined by Buzzi\cite{Buzzi2011} in their original algorithm. This algorithm removes a single anomaly per function call until all anomalies have been removed.

\subsubsection{Mean Standard Deviation Algorithm}
Figure \ref{algorithm:sd-mean} showcases a function for standard deviaion around the mean. This is a common approach and checks if all datapoints are in range. If they are outside of the range, they are added to the anomalies array and the anomalies array is returned after the function is finished.

\subsubsection{Median Standard Deviation Algorithm}
Figure \ref{algorithm:sd-median} showcases a function for standard deviaion around the median. This is a common approach and checks if all datapoints are in range. If they are outside of the range, they are added to the anomalies array and the anomalies array is returned after the function is finished.

\section{Data Collection Methodology}
\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=12cm]{auction-house-before.png}
    \caption{Auction house screenshot before image processing.}
    \label{figure:auction-house-before}
\end{figure*}

\subsection{Internal Data}
\noindent The best method of gathering data, is to contact the developers directly. The developer's internal policy could impact the ability to get the raw data from them directly. They may log a lot more data than is publicly accessible via the marketplace, for example, the internal data could contain account identifying information.

\subsection{Public Data}
\noindent In Korea, a public resource exist which allows users to view trasactional data across the entire auction house\cite{LostArkStove2019}. However, this website is inaccessible without name and age verification, due Korea's Game Industry Promotion Act\cite{GIPA2020kr}.

\subsection{In-game Data}
\noindent Another method for data collection surrounds screenshotting pages from the Sale History of Gems from Lost Ark's Marketplace, as seen in Figure \ref{figure:auction-house-before}. Then, the screenshot is cropped to remove the Gem Name, Starting Bid and Quality fields and converted to a greyscale image along with other image manipulation techniques to ensure clear text, shown in Figure \ref{figure:auction-house-after} (the algorithm which produces this processed image is shown in Figure \ref{algorithm:image-formatter}).

Optical character recognition (OCR) is then used on the image and checked automatically for any errors (see Figure \ref{algorithm:image-processor}). The data can then be randomly sampled and manually reviewed to ensure the accuracy of the data following this process. Due to the nature in which this algorithm works, unreadable data will be logged in the saved file as a "-" alongside its page and entry numbers, making the issue much quicker to rectify.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.25]{auction-house-after}
    \caption{Auction house screenshot after image processing.}
    \label{figure:auction-house-after}
\end{figure}

Removing the name is done as this has no bearing on the price of a gem. Removing the Starting Bid field is done because the Starting Bid is optional when adding a gem to the marketplace, it is also not indicative of RMT. Instead, for the RMT transaction to occur correctly, Gems have a Buy Now Price set to a specific value. Removing the Quality field is done since gems don't have a quality value; this is always "-".

A consideration which could also affect the price of Gems is the "Gem Effect". This is a specific ability that Gem does to impact a character's Damage or Cooldown Time on a specific move. However, the Gem Effect is optionally rerolled within the game player. Rerolling requires the use of Silver, a much easier resource to gather, so this doesn't have a huge impact on the price of the gem.

Using this data, humans can easily recognise when a sale price for a specific level/tier gem is too high compared to other gems being sold at around the same time.

Overall, after collecting data for the period 25th August 2022 until 29th December 2022*, a total of 14,928 gems were sold on the EU West market within Lost Ark and tracked for this paper (see Figure \ref{graph:gem-datasize}).

\section{Validation and Verification}
\noindent Without having a labelled dataset, it will be difficult to accurately verify anomalous transactions using an algorithm. However, by utilising different algorithms for the testing of data, it is possible to check overlapping anomalies between different algorithms. This will allow for the verification that a data point is anomalous.

The algorithms can also be validated by running them multiple times. As computation is likely to be nearly instantaneous running on modern machines, running it hundereds of times, the time to run the algorithms can be calculated more accurately. To ensure accuracy and authenticity

\subsection{Verifying Algorithms by Comparing Results of Different Algorithms}
\noindent Comparing the results is a way in which the data can be validated. By assigning each gem with a unique value based on its location within the database we can check each gem for its occurrence as an anomaly across multiple algorithms. An example in Python would look like \ref{algorithm:algorithm-comparison}

This method, however, suffers from a problem where anomalies which aren't detected by any algorithm won't be verified at all, leading to a reduced accuracy rate.

\section{Considerations}
\subsection{Legal}
\noindent Smilegate, the developers of Lost Ark, didn't explicitly give permission for their dataset to use be used in this paper, however, it doesn't breach their Code of Concuct\cite{AmazonGamesCOC}. This dataset is also available

\subsection{Ethical}
\noindent The data processed in the paper is gathered directly from Lost Ark's public marketplace where Smilegate likely already follow data anomyization practices\cite{Gruschka2018}. The data gathered doesn't contain identifiable information and therefore conforms to both General Data Protection Regulation (GDPR) and the Nuremberg Code\cite{Nuremberg1947}. As this papers research is carried out at Falmouth University, it also follows the Research Policy\cite{FalmouthEthicsPolicy2022}.

There is also a concern that as this is to do with a form of financial data, that people may rely on RMT in order to make money in thier respective counties. This could pose an issue as the plan is not to ruin someone's livlihood. On the other hand, RMT is not allowed by the game and could be comparable to turning towards crime during hardship.

\subsection{Professional}
\noindent Professionally, it's important that this paper follows the BCS Code of Conduct\cite{BCSCodeOfConduct2022}. This includes working for the public interest by outlining solutions to current problems rather than ways to make current problems worse.

\section{Results and Analysis}
\noindent The data shown has a GemID. This GemID coincides with two metrics each gem has, Tier and Level. Tier is represented via a T2 or T3 at the beginning of the ID and Level is represeted by L01-L10 at the end of the ID. This means that a Tier 2 Level 7 gem would be represented as T2L07 and a Tier 3 Level 10 gem would be T3L10. 

The data shown in Figure \ref{graph:gem-datasize} showcases the total size of the data for each gem type sold. There is a stark drop off in the sample size for T2L08-T2L10 gems. This will be due to Tier 2 being replaced by Tier 3 through game progression. In my personal experience, this was around the time I started needing T2L08 gems, explaining the lack of these types of gems. There is also a drop off related to T3L09 and T3L10 gems which could be attributed to their difficulty to gather as they don't drop naturally and must be created by merging 3x T3L08 and 3x T3L09 gems respectively.

Figure \ref{graph:anomalies-per-algorithm} showcases the proportion of anomalies found for each Gem ID. This data shows that T2L01 gems are likely used more often for fraudulent transactions. Due to thier low value in-game however, players typically spend 1 Gold each on them. Therefore, it is likely that the low cost gems were detected as anomalies by the 2T and CleverSD algorithms, this in and of itself could highlight how these gems being sold at vast range of prices actually increases their detection for these algorithms.
The graph also shows a huge proportion of T2L05 being detected by the CleverSD and 2T algorithms. This could be attributed to fraudulent transactions.
The Median SD algorithm used detected anomalies a comparitably large proportion of Level 10 gems; T2L10 and T3L10. For T3L10, this could be attributed to the high range in price for these gems as shown in Figure \ref{graph:gem-sale-price}. As for T2L10, this could be attributed to an insufficient data size when compared with other gems, as shown in Figure \ref{graph:gem-datasize}.

Figure \ref{graph:time-taken-per-algorithm} shows that overall, CleverSD is extremely slow when compared to the other three algorithms. This is unperformant, especially when considering the data in Figure \ref{graph:anomalies-per-algorithm} where CleverSD and 2T both found the same amount of anomalies for each Gem ID.

Overall, these graphs help highlight that CleverSD and 2T both itentified a nearly identical number of anomalous data points, suggesting that the two algorithms work very similarly. However, this doesn't guarentee accuracy and could be down to over-reporting of anomalous findings, especially when compared with SD Mean and SD Median. On average, SD Median found more anomalous data points than SD Mean, which could highlight how the data is skewed especially when looking at the sale prices shown in Figure \ref{graph:gem-sale-price}. Overall, its difficult to assess accuracy due to the lack of a labelled dataset.

\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=12cm]{graph-gem-datasize.png}
    \caption{Bar chart showing the number of gems recorded for each gem ID. R shown in Figure \ref{algorithm:r-gem-datasize}}
    \label{graph:gem-datasize}
\end{figure*}

\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=12cm]{graph-anomalies-per-algorithm.png}
    \caption{Bar chart showing the percentage of anomalies found for each gem depending upon the algorithm. R shown in Figure \ref{algorithm:r-anomalies-per-algorithm}}
    \label{graph:anomalies-per-algorithm}
\end{figure*}

\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=12cm]{graph-compute-time-per-algorithm.png}
    \caption{Bar chart showing the average computation time per 1000 gems for each algorithm (lower is better). R shown in Figure \ref{algorithm:r-time-taken-per-element}}
    \label{graph:time-taken-per-algorithm}
\end{figure*}

\section{Disccussion}
\noindent Looking at the data generated, it is clear that the hypotheses were along the right lines, but didn't paint the full picture. For the first research question, "Which anomaly detection algorithm is the most performant for peer-to-peer transactions within MMOs?" its clear than CleverSD was a huge way off in its computational time, performing the slowest by a substantial margin. The Mean SD was the quickest, beating out the Median SD algorithms, due to the lack of recursion.

For Hypothesis 2, "Which algorithm will detect the most anomalous data points?", the data shows that CleverSD, alongside 2T, identified the most anomalous datapoints, however, this is due to the recursive nature of the algorithms when compared with Mean and Median SD. The lack of tuning the algorithms or the small datasets used could also factor into the detection.

\printbibliography
\begin{appendices}
    \begin{appendices}
        \section{Acknowledgements}
        \label{appendix:aknowledgements}
        I would like to thank my supervisor John Speakman for support during this project. I would also like to thank Michael Scott for his work in creating this module, and his work towards improving the University as a whole. I'd also like to thank all my lecturers at Falmouth University who have supported my learning over the last 3 years, as well as their openness to change based on what the student body is after.
    \end{appendices}

    \begin{appendices}
        \section{Reflective Addendum}
        \label{appendix:reflection}
        I am happy overall with the progress which I have made on this dissertation, however, it hasn't been without its issues and looking back, I would have done a lot differently if I was to start this project fresh.

        \subsection{Change the dataset}
        One of the major issues I have faced has been regarding the dataset used. When I started this project, I had been playing Lost Ark with friends for the prior couple of months and wanted to solve a problem that I felt had plagued the game. I did contact the developers, hoping I could access this data as part of my dissertation but I never recieved a response. Instead of going ahead with the data I gathered, it would have been a great idea to use an open and pre-labelled dataset from another game; these overall are difficult to come by and I wouldn't have the same connection and understanding of a different dataset.

        \subsection{Look at different algorithms}
        Another thing I would have liked to look into would have been other methods of anomaly detection. Originally this was the plan, but to help manage scope and not overwork myself for this project - and other committments I had - I chose to focus on a small subsection of algorithms, which focused solely around Standard Deviation. I would have liked to have Machine Learning or Autoencoders but this felt out of reach during the middle of the project.

        \subsection{Overall dislike of academic writing}
        One of my biggest issues with school in general, is academic writing. I don't enjoy writing essays - and I'm sure not many people do - but that has been a big blocker for me personally. It led to me developing my algorithms quite early, learning about OCR and detection algorithms without putting a huge amount of thought into this paper as a whole. It also led to me focusing on my other, more practical, modules or work experience through internships. The literature review was the only part I particularly found enjoyable as I found the academic reading to be interesting. This was due to me learning new things, something I enjoy doing.

        \subsection{Conclusion}
        Overall, I believe that the project has gone well, I feel happy and content with what I have produced. Whilst it could have been better, it also could have gone much worse. With a little more thought into time management and planning, I could have produced something better. 
    \end{appendices}

    \section{Statistics Addendum}
    \label{appendix:statistics-addendum}

    \lstinputlisting[language=r]{algorithms/r/gem-datasize.R}
    \begin{figure}[hbt!]
        \caption{R Code which displays the gem datasize graph shown in Figure \ref{graph:gem-datasize}.}
        \label{algorithm:r-gem-datasize}
    \end{figure}

    \lstinputlisting[language=r]{algorithms/r/time-taken-per-element.R}
    \begin{figure}[hbt!]
        \caption{R Code which displays the average time taken per 100 elements graph shown in Figure \ref{graph:time-taken-per-algorithm}.}
        \label{algorithm:r-time-taken-per-element}
    \end{figure}

    \lstinputlisting[language=r]{algorithms/r/anomalies-per-algorithm.R}
    \begin{figure}[hbt!]
        \caption{R Code which displays the graph showing number of anomalies generated from each algorithm in Figure \ref{graph:anomalies-per-algorithm}.}
        \label{algorithm:r-anomalies-per-algorithm}
    \end{figure}

    \lstinputlisting[language=r]{algorithms/r/gem-sale-price.R}
    \begin{figure}[hbt!]
        \caption{R Code which displays the graph showing number of anomalies generated from each algorithm in Figure \ref{graph:gem-sale-price}.}
        \label{algorithm:r-gem-sale-price}
    \end{figure}

    \section{Testing Addendum}
    \label{appendix:testing-addendum}

    \section{Data Collection Code Samples}
    \label{appendix:collection-code}
    
    \lstinputlisting[language=python]{algorithms/python/compare-algorithms.py}
    \begin{figure}[hbt!]
        \caption{Comparison Algorithm.}
        \label{algorithm:algorithm-comparison}
    \end{figure}

    \lstinputlisting[language=python]{algorithms/python/clever_sd.py}
    \begin{figure}[hbt!]
        \caption{CleverSD Algorithm.}
        \label{algorithm:clever-sd}
    \end{figure}

    \section{Anomaly Detection Algorithms}
    \label{appendix:detection-code}
    
    \lstinputlisting[language=python]{algorithms/python/twostage_sd.py}
    \begin{figure}[hbt!]
        \caption{2T Algorithm.}
        \label{algorithm:2t}
    \end{figure}
    
    \lstinputlisting[language=python]{algorithms/python/mean_sd.py}
    \begin{figure}[hbt!]
        \caption{Standard Deviation (Mean) Algorithm.}
        \label{algorithm:sd-mean}
    \end{figure}
    
    \lstinputlisting[language=python]{algorithms/python/median_sd.py}
    \begin{figure}[hbt!]
        \caption{Standard Deviation (Median) Algorithm.}
        \label{algorithm:sd-median}
    \end{figure}

    \lstinputlisting[language=python]{algorithms/python/image-formatter.py}
    \begin{figure}[hbt!]
        \caption{Image Formatter Algorithm.}
        \label{algorithm:image-formatter}
    \end{figure}

    \lstinputlisting[language=python]{algorithms/python/image-processor.py}
    \begin{figure}[hbt!]
        \caption{Image Processing Algorithm.}
        \label{algorithm:image-processor}
    \end{figure}
\end{appendices}

\end{document}